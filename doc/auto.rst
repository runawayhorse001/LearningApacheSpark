
.. _auto:

===========================================
Automation for Cloudera Distribution Hadoop  
===========================================

CDH (Cloudera Distribution Hadoop) is the most complete, tested, and widely deployed distribution of Apache Hadoop. A lot of small or middle size companies are using CHD. While Cloudera does not support IPython or Jupyter notebooks on CDH and the Cloudera Data Science Workbench is expensive, many compaies are using ``CDH+zeppelin`` or ``CDH+jupyterhub`` infrastructure. This infrastructure works pretty well, but it's inconvenient for Data Engineer or Data Scientist to do automation during the production process.  This chapter will cover how to use  `Jinja2`_, `spark sql`_ and `ML Pipelines`_ to implement the automation for Cloudera Distribution Hadoop. 

Automation Pipeline 
+++++++++++++++++++

The automation pipeline mainly contains two parts: 
	
 1. `Jinja2`_ + `spark sql`_ for data clean and manipulation automation
 2. `ML Pipelines`_ for Machine  Leanring automation 


.. _fig_pipline_flow:
.. figure:: images/pipline_flow.png
	:align: center


Data Clean and Manipulation Automation
++++++++++++++++++++++++++++++++++++++

Jinja 2
-------

Jinja is a modern and designer-friendly templating language for Python, modelled after Djangoâ€™s templates. Use Jinja2 to generate 
SQL query will need two steps:

1. Get template

.. code-block:: python

	temp = """
	    SELECT project, timesheet, hours
	    FROM timesheet
	    WHERE user_id = {{ user_id }}
	    {% if project_id %}
	    AND project_id = {{ project_id }}
	    {% endif %}
	"""

2. render the tempalte 

.. code-block:: python

	args = {"user_id": u"runawayhorse", 
	        "project_id": 123}

	query=  Template(temp).render(args)

	print(query)

Then, you will get the following SQL query:

.. code-block:: sql


    SELECT project, timesheet, hours
    FROM timesheet
    WHERE user_id = runawayhorse
    
    AND project_id = 123

.. admonition:: Note

   The Jinja is smart then you think. If you try this  

	.. code-block:: python

		args = {"user_id": u"runawayhorse"}

		query=  Template(temp).render(args)

		print(query)

	Then, you will get the following SQL query:

	.. code-block:: sql


	    SELECT project, timesheet, hours
	    FROM timesheet
	    WHERE user_id = runawayhorse

If you have a long query, you can use Iinja ``get_template`` to read the tempalte:

.. code-block:: python

	import os
	from jinja2 import Template
	from jinja2 import Environment, FileSystemLoader


	path = os.path.abspath(os.path.join(sys.path[0]))
	try:
	    os.mkdir(path)
	except OSError:
	    pass
	os.chdir(path)
	print(path)

	jinja_env = Environment(loader=FileSystemLoader(path))
	template = jinja_env.get_template('test.sql')
	query = template.render(states=states)
	print(query)

with ``test.sql`` file is as follows:

.. code-block:: python

	select id
	{% for var in states %}
	, (CASE WHEN (off_st = '{{var}}') THEN 1 ELSE 0 END)  AS off_st_{{var}}
	{% endfor %}
	FROM table1

Then you will get the following query:

.. code-block:: sql

	select id

	, (CASE WHEN (off_st = 'MO') THEN 1 ELSE 0 END)  AS off_st_MO

	, (CASE WHEN (off_st = 'KS') THEN 1 ELSE 0 END)  AS off_st_KS

	, (CASE WHEN (off_st = 'KY') THEN 1 ELSE 0 END)  AS off_st_KY

	, (CASE WHEN (off_st = 'OH') THEN 1 ELSE 0 END)  AS off_st_OH

	FROM table1




Spark SQL
---------

Spark SQL at here will be called to excute SQL or HiveQL queries which generated by Jinjia2 on existing warehouses.

.. code-block:: python
    
	# without output
	spark.sql(query)

	# with output
	df = spark.sql(query)



ML Pipeline Automation
++++++++++++++++++++++

I will not cover the details of the ML Pipeline at here, the interested reader is referred to  `ML Pipelines`_ . The The main steps for defining the stages are as follows:

.. code-block:: python

	scalering ='Standard'

	from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler
	if scalering=='Normal':
	    scaler = Normalizer(inputCol="features", outputCol="scaledFeatures", p=1.0)
	elif scalering=='Standard':
	    scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures",
	                            withStd=True, withMean=False)
	else:
	    scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")

	from pyspark.ml.feature import StringIndexer
	# Index labels, adding metadata to the label column
	labelIndexer = StringIndexer(inputCol='label',
	                             outputCol='label').fit(transformed)

	from pyspark.ml.feature import IndexToString
	# Convert indexed labels back to original labels.
	labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
	                               labels=labelIndexer.labels)


	from pyspark.ml.classification import LogisticRegression
	ml = LogisticRegression(featuresCol='scaledFeatures', labelCol='label', maxIter=100, regParam=0.01, elasticNetParam=0.6)

	# Chain indexers and tree in a Pipeline
	pipeline_model = Pipeline(stages=[scaler,labelIndexer,ml,labelConverter])

	# Train model.  This also runs the indexers.
	model = pipeline_model.fit(trainingData)

	# Make predictions.
	predictions = model.transform(testData)


Save and Load PipelineModel
+++++++++++++++++++++++++++

.. code-block:: python

	# save PipelineModel
	model.write().overwrite().save(out_path)

	# load PipelineModel
	from pyspark.ml import PipelineModel

	model = PipelineModel.load(out_path)


Ingest Results Back into Hadoop 
+++++++++++++++++++++++++++++++


.. code-block:: python

	df.createOrReplaceTempView("temp_table")

	query = '''
	create table database_name.prediction_{{dt}} AS
	SELECT *
	FROM temp_table
	'''
	output = Template(query).render(dt=dt)
	spark.sql(output)




.. _Jinja2: https://jinja.palletsprojects.com/en/2.10.x/
.. _spark sql: https://spark.apache.org/sql/
.. _ML Pipelines: https://spark.apache.org/docs/latest/ml-pipeline.html